# AgentOS WebUI 一键启动完整演示

## 🎬 完整启动流程演示

以下是从零开始到 WebUI 完全可用的完整流程：

---

## 场景：全新环境首次安装

**前提条件：**
- ✅ 已安装 Python 3.13+
- ✅ 已安装 uv
- ❌ 未安装 Ollama
- ❌ 未安装依赖
- ❌ 未初始化数据库

---

### 步骤 1: 执行启动命令

```bash
$ cd /path/to/AgentOS
$ uv run agentos webui start
```

---

### 步骤 2: 环境检测

```
═══════════════════════════════════════
     AgentOS WebUI 启动检查
═══════════════════════════════════════

═══ 环境检测 ═══
✓ Python 版本: 3.13.2
✓ uv 工具: uv 0.5.9 (0652800cb 2024-12-13)
```

**自动完成：** ✅
**耗时：** < 1 秒

---

### 步骤 3: Provider 检测

```
═══ 检查本地 AI Provider ═══
┏━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┓
┃ Provider   ┃ 状态          ┃ 信息            ┃
┣━━━━━━━━━━━━╋━━━━━━━━━━━━━━╋━━━━━━━━━━━━━━━━┫
┃ Ollama     ┃ ✗ 不可用      ┃ 命令不存在      ┃
┃ LM Studio  ┃ ✗ 不可用      ┃ 未运行          ┃
┃ llama.cpp  ┃ ✗ 不可用      ┃ 命令不存在      ┃
┗━━━━━━━━━━━━┻━━━━━━━━━━━━━━┻━━━━━━━━━━━━━━━━┛

⚠️  未检测到本地 AI Provider
建议安装 Ollama 以使用本地模型
您也可以跳过安装，稍后使用云端 API (OpenAI/Anthropic 等)

是否安装 Ollama? [Y/n]:
```

**用户操作：** 按回车（默认 Y）

```
正在安装 Ollama...
⠋ 下载并安装 Ollama...
✓ Ollama 安装成功

提示: 您可以运行以下命令下载模型:
  ollama pull llama3.2
  ollama pull qwen2.5
```

**耗时：** 3-5 分钟

---

### 步骤 4: Provider 配置

```
配置 Ollama...
⚠️  Ollama 服务未运行
请输入 Ollama 服务端口 [11434]:
```

**用户操作：** 按回车（使用默认端口）

```
是否启动 Ollama 服务 (端口 11434)? [Y/n]:
```

**用户操作：** 按回车（默认 Y）

```
正在启动 Ollama 服务 (端口 11434)...
⠋ 等待服务就绪...
✓ Ollama 服务启动成功

验证 Ollama 连接...
✓ 连接成功，但未安装模型

更新 Provider 配置...
✓ 已更新配置: ollama -> http://127.0.0.1:11434
配置文件: ~/.agentos/config/providers.json
```

**耗时：** 5-10 秒

---

### 步骤 5: 依赖检测

```
═══ 检查 Python 依赖 ═══
✓ 已安装 4 个依赖包
⚠️  发现 3 个缺失的依赖包:
  ✗ fastapi
  ✗ uvicorn
  ✗ websockets

是否执行 'uv sync' 安装依赖? [Y/n]:
```

**用户操作：** 按回车（默认 Y）

```
正在安装依赖...
⠋ 执行 uv sync...
✓ 依赖安装成功
```

**耗时：** 30-60 秒

---

### 步骤 6: 数据库准备

```
═══ 检查数据库 ═══
✗ 数据库文件不存在: store/registry.sqlite
是否创建数据库? [Y/n]:
```

**用户操作：** 按回车（默认 Y）

```
正在创建数据库...
⠋ 初始化数据库...
✓ 数据库已创建: store/registry.sqlite

检查数据库迁移...

发现 33 个待执行的迁移:
  - v01
  - v02
  - v03
  ...
  - v33

是否执行数据库迁移? [Y/n]:
```

**用户操作：** 按回车（默认 Y）

```
正在执行迁移...
⠋ 应用数据库迁移...
✓ 成功应用 33 个迁移
```

**耗时：** 2-3 秒

---

### 步骤 7: 启动 WebUI

```
✓ 所有检查通过，准备启动 WebUI

═══════════════════════════════════════

🚀 Starting WebUI at 127.0.0.1:8080...

✅ WebUI started successfully
🌐 URL: http://127.0.0.1:8080
📋 Logs: ~/.agentos/webui.log

提示: 使用 'agentos webui stop' 停止服务
```

**耗时：** 2-3 秒

---

### 步骤 8: 访问 WebUI

```bash
# 浏览器打开
$ open http://127.0.0.1:8080
```

或者直接在浏览器中访问 `http://127.0.0.1:8080`

---

### 步骤 9: 使用 Chat 功能

1. 点击左侧菜单 **Chat**
2. 选择 Provider: **Ollama**
3. 输入消息：`你好，请介绍一下自己`
4. 发送 → **正常返回响应** ✅

**Ollama 已配置并可用！** 🎉

---

## 📊 完整流程总结

| 阶段 | 操作 | 耗时 | 用户交互 |
|------|------|------|----------|
| 1. 环境检测 | 自动 | < 1s | 无 |
| 2. Provider 检测 | 自动 | < 1s | 无 |
| 3. Ollama 安装 | 自动 | 3-5min | 确认安装 |
| 4. Provider 配置 | 半自动 | 5-10s | 配置端口 + 确认启动 |
| 5. 依赖安装 | 自动 | 30-60s | 确认安装 |
| 6. 数据库准备 | 自动 | 2-3s | 确认创建 + 确认迁移 |
| 7. WebUI 启动 | 自动 | 2-3s | 无 |
| **总计** | | **4-7 分钟** | **5 次确认** |

---

## 🎯 第二次启动（环境已就绪）

```bash
$ uv run agentos webui start

═══════════════════════════════════════
     AgentOS WebUI 启动检查
═══════════════════════════════════════

═══ 环境检测 ═══
✓ Python 版本: 3.13.2
✓ uv 工具: uv 0.5.9

═══ 检查本地 AI Provider ═══
┏━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┓
┃ Provider   ┃ 状态          ┃ 信息            ┃
┣━━━━━━━━━━━━╋━━━━━━━━━━━━━━╋━━━━━━━━━━━━━━━━┫
┃ Ollama     ┃ ✓ 可用        ┃ v0.15.2         ┃
┗━━━━━━━━━━━━┻━━━━━━━━━━━━━━┻━━━━━━━━━━━━━━━━┛

✓ 使用 Provider: Ollama

配置 Ollama...
✓ Ollama 服务已运行

验证 Ollama 连接...
✓ 连接成功，发现 3 个模型

更新 Provider 配置...
✓ 已更新配置: ollama -> http://127.0.0.1:11434

═══ 检查 Python 依赖 ═══
✓ 已安装 7 个依赖包
✓ 所有依赖包已安装

═══ 检查数据库 ═══
✓ 数据库文件存在: store/registry.sqlite
检查数据库迁移...
✓ 数据库已是最新版本

✓ 所有检查通过，准备启动 WebUI

═══════════════════════════════════════

🚀 Starting WebUI at 127.0.0.1:8080...

✅ WebUI started successfully
🌐 URL: http://127.0.0.1:8080
📋 Logs: ~/.agentos/webui.log

提示: 使用 'agentos webui stop' 停止服务
```

**总耗时：** 2-3 秒 ⚡️
**用户交互：** 0 次 🎉

---

## 🔧 自定义配置示例

### 使用自定义端口

```bash
$ uv run agentos webui start

配置 Ollama...
⚠️  Ollama 服务未运行
请输入 Ollama 服务端口 [11434]: 9090     # 输入自定义端口
是否启动 Ollama 服务 (端口 9090)? [Y/n]: y

正在启动 Ollama 服务 (端口 9090)...
✓ Ollama 服务启动成功

验证 Ollama 连接...
✓ 连接成功，发现 3 个模型

更新 Provider 配置...
✓ 已更新配置: ollama -> http://127.0.0.1:9090
```

### 使用不同的 WebUI 端口

```bash
$ uv run agentos webui start --port 9999

# WebUI 将在 http://127.0.0.1:9999 启动
```

---

## 🚫 常见问题处理

### 问题 1: 端口被占用

```
正在启动 Ollama 服务 (端口 11434)...
✗ 端口 11434 已被占用

请输入 Ollama 服务端口 [11434]: 11435
是否启动 Ollama 服务 (端口 11435)? [Y/n]: y

正在启动 Ollama 服务 (端口 11435)...
✓ Ollama 服务启动成功
```

### 问题 2: 连接验证失败

```
验证 Ollama 连接...
✗ 连接失败: Connection refused

⚠️  Ollama 服务可能未完全启动
建议:
  1. 等待几秒后重试
  2. 手动启动: ollama serve
  3. 检查防火墙设置

是否继续启动 WebUI? [Y/n]: n
✗ 已取消启动
```

### 问题 3: 无可用模型

```
验证 Ollama 连接...
✓ 连接成功，但未安装模型

⚠️  提示: 下载推荐模型
运行以下命令:
  ollama pull llama3.2
  ollama pull qwen2.5

是否继续启动 WebUI? [Y/n]: y
（继续启动，稍后在 WebUI 中下载模型）
```

---

## 💡 最佳实践

### 首次安装建议

1. **预先下载模型**
   ```bash
   # 先安装 Ollama
   curl -fsSL https://ollama.com/install.sh | sh

   # 下载常用模型
   ollama pull llama3.2
   ollama pull qwen2.5

   # 然后启动 AgentOS
   uv run agentos webui start
   ```

2. **检查系统资源**
   - 确保有足够磁盘空间（模型通常 4-10GB）
   - 确保有足够内存（推荐 8GB+）

3. **使用默认配置**
   - 首次安装建议使用默认端口
   - 熟悉后再自定义配置

### 日常使用建议

1. **快速启动**
   ```bash
   uv run agentos webui start
   # 2-3 秒即可启动
   ```

2. **后台运行**
   ```bash
   # WebUI 默认在后台运行
   # 查看日志
   tail -f ~/.agentos/webui.log
   ```

3. **停止服务**
   ```bash
   uv run agentos webui stop
   ```

---

## 🎉 总结

通过这套完整的启动检查和配置系统：

✅ **首次安装：** 4-7 分钟，5 次确认
✅ **日常启动：** 2-3 秒，无需交互
✅ **开箱即用：** Chat 功能直接可用
✅ **用户友好：** 清晰的提示和进度
✅ **错误恢复：** 完善的故障处理

**真正实现了一键启动，零配置使用！** 🚀
