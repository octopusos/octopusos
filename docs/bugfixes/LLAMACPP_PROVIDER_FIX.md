# llama.cpp Provider æ”¯æŒä¿®å¤

## ğŸ› é—®é¢˜æè¿°

**ç”¨æˆ·æŠ¥å‘Š**:
- åœ¨ WebUI ä¸­é€‰æ‹© `llama.cpp` provider
- å‘é€æ¶ˆæ¯æ—¶æŠ¥é”™ï¼š`âš ï¸ Model unavailable: âœ— Ollama unreachable: HTTPConnectionPool(host='localhost', port=11434)`
- ç³»ç»Ÿå°è¯•è¿æ¥ Ollama çš„ 11434 ç«¯å£ï¼Œè€Œä¸æ˜¯ llama.cpp çš„ç«¯å£

**æœŸæœ›è¡Œä¸º**:
- é€‰æ‹© llama.cpp provider æ—¶ï¼Œåº”è¯¥è¿æ¥åˆ° llama.cpp çš„å®é™…ç«¯å£
- æ”¯æŒ instance-based provider æ ¼å¼ï¼ˆä¾‹å¦‚ `llamacpp:qwen3-coder-30b`ï¼‰

---

## ğŸ” æ ¹æœ¬åŸå› 

### é—®é¢˜ 1: Provider ç¡¬ç¼–ç 

**agentos/core/chat/engine.py**:
```python
# Line 164 - ç¡¬ç¼–ç  provider æ˜ å°„
provider = "ollama" if model_route == "local" else "openai"
```

- `_stream_response()` å’Œ `_invoke_model()` å®Œå…¨å¿½ç•¥ session metadata ä¸­çš„ provider è®¾ç½®
- æ‰€æœ‰ local æ¨¡å‹éƒ½è¢«å¼ºåˆ¶æ˜ å°„åˆ° ollama

### é—®é¢˜ 2: Adapter ä¸æ”¯æŒ llamacpp

**agentos/core/chat/adapters.py**:
```python
def get_adapter(provider: str, model: Optional[str] = None):
    if provider == "ollama":
        return OllamaChatAdapter(model=model)
    elif provider == "openai":
        return OpenAIChatAdapter(model=model)
    else:
        raise ValueError(f"Unknown provider: {provider}")
```

- `get_adapter()` åªæ”¯æŒ `ollama` å’Œ `openai`
- ä¸æ”¯æŒ `llamacpp` æˆ– `lmstudio`
- ä¸æ”¯æŒ instance-based æ ¼å¼ï¼ˆ`provider:instance-name`ï¼‰

### é—®é¢˜ 3: API ç«¯ç‚¹ä¸å…¼å®¹

- Ollama API: `/api/chat`, `/api/tags`
- llama.cpp API: `/v1/chat/completions`, `/v1/models`, `/health` (OpenAI å…¼å®¹)
- ä½¿ç”¨ OllamaChatAdapter è®¿é—® llama.cpp ä¼šå¤±è´¥ï¼ˆ404ï¼‰

---

## âœ… ä¿®å¤æ–¹æ¡ˆ

### 1. ä¿®æ”¹ ChatEngine è¯»å– Session Metadata

**agentos/core/chat/engine.py**:

```python
def _stream_response(self, session_id: str, context_pack: Any, model_route: str = "local"):
    # Get session to read provider/model preferences
    session = self.chat_service.get_session(session_id)

    # Determine provider from session metadata (with fallback)
    provider = session.metadata.get("provider")
    if not provider:
        provider = "ollama" if model_route == "local" else "openai"

    # Get model name if specified
    model = session.metadata.get("model")

    logger.info(f"Using provider: {provider}, model: {model}")

    adapter = get_adapter(provider, model)
```

åŒæ ·ä¿®æ”¹ `_invoke_model()` æ–¹æ³•ã€‚

### 2. æ‰©å±• get_adapter æ”¯æŒå¤šç§ Provider

**agentos/core/chat/adapters.py**:

```python
def get_adapter(provider: str, model: Optional[str] = None) -> ChatModelAdapter:
    """Get chat model adapter

    Args:
        provider: Provider ID (e.g., "ollama", "llamacpp", "llamacpp:instance-name")
        model: Optional model name override
    """
    # Parse provider:instance format
    if ":" in provider:
        provider_type, instance_id = provider.split(":", 1)
    else:
        provider_type = provider
        instance_id = None

    # Handle llama.cpp (OpenAI-compatible)
    if provider_type == "llamacpp":
        model = model or "local-model"
        base_url = "http://127.0.0.1:8080"

        # Get actual endpoint from registry if instance specified
        if instance_id:
            try:
                from agentos.providers.registry import ProviderRegistry
                registry = ProviderRegistry.get_instance()
                provider_obj = registry.get(f"llamacpp:{instance_id}")
                if provider_obj and hasattr(provider_obj, 'endpoint'):
                    base_url = provider_obj.endpoint
                    logger.info(f"Using llamacpp endpoint: {base_url}")
            except Exception as e:
                logger.warning(f"Failed to get llamacpp instance endpoint: {e}")

        # llama.cpp uses OpenAI-compatible API
        return OpenAIChatAdapter(model=model, base_url=f"{base_url}/v1", api_key="dummy")
```

åŒæ ·æ·»åŠ äº†å¯¹ `lmstudio` çš„æ”¯æŒã€‚

### 3. ä¿®æ”¹ OpenAIChatAdapter æ”¯æŒ Local Services

**ä¿®æ”¹ health_check()**:
```python
def health_check(self) -> tuple[bool, str]:
    """Check OpenAI availability"""
    # For custom base_url (llama.cpp, lmstudio), check endpoint instead of API key
    if self.base_url:
        try:
            import requests
            # Try health endpoint
            health_url = self.base_url.replace("/v1", "/health")
            response = requests.get(health_url, timeout=5)
            if response.status_code == 200:
                return True, f"âœ“ Local Model ({self.model})"

            # Fallback: try models endpoint
            models_url = f"{self.base_url}/models"
            response = requests.get(models_url, timeout=5)
            if response.status_code == 200:
                return True, f"âœ“ Local Model ({self.model})"

            return False, f"âœ— Service error ({response.status_code})"
        except Exception as e:
            return False, f"âœ— Service unreachable: {str(e)}"

    # For OpenAI API (original logic)
    ...
```

**ä¿®æ”¹ generate() å’Œ generate_stream()**:
```python
# Only check API key for actual OpenAI (not for local services)
if not self.api_key and not self.base_url:
    return "âš ï¸ Error: OPENAI_API_KEY not configured"
```

### 4. ä¿®æ”¹ OllamaChatAdapter æ”¯æŒè‡ªå®šä¹‰ Base URL

```python
def __init__(self, model: str = "qwen2.5:14b", base_url: Optional[str] = None):
    """Initialize Ollama adapter

    Args:
        model: Model name
        base_url: Base URL (defaults to OLLAMA_HOST env var)
    """
    self.model = model
    self.host = base_url or os.environ.get("OLLAMA_HOST", "http://localhost:11434")
```

---

## ğŸ§ª æµ‹è¯•ç»“æœ

### Smoke Test

åˆ›å»ºäº† `test_adapter_simple.py` è¿›è¡Œæµ‹è¯•ï¼š

```bash
$ python3 test_adapter_simple.py

Testing llama.cpp adapter...

1. Creating adapter with provider='llamacpp:qwen3-coder-30b'...
   âœ“ Adapter created: OpenAIChatAdapter
   - Base URL: http://127.0.0.1:11435/v1
   - Model: local-model

2. Testing health check...
   Status: âœ“ Local Model (local-model)
   âœ“ Health check PASSED

3. Testing generate (non-streaming)...
   Response: Test successful
   âœ“ Generate PASSED

4. Testing generate_stream...
   1, 2, 3
   Counting complete! ğŸ‰
   âœ“ Streaming PASSED (10 chunks)

============================================================
âœ“ ALL TESTS PASSED
============================================================
```

### API ç«¯ç‚¹éªŒè¯

```bash
# Health check
$ curl http://127.0.0.1:11435/health
{"status":"ok"}

# Models endpoint
$ curl http://127.0.0.1:11435/v1/models
{
  "models": [
    {
      "name": "Qwen3-Coder-30B-A3B-Instruct-UD-Q8_K_XL.gguf",
      ...
    }
  ]
}

# Chat completion
$ curl -X POST http://127.0.0.1:11435/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "Qwen3-Coder-30B-A3B-Instruct-UD-Q8_K_XL.gguf",
    "messages": [{"role": "user", "content": "Hello"}],
    "stream": false
  }'
{
  "choices": [
    {
      "finish_reason": "stop",
      "message": {
        "role": "assistant",
        "content": "Hello! How can I help you today?"
      }
    }
  ],
  ...
}
```

---

## ğŸ“Š Provider æ”¯æŒçŸ©é˜µ

| Provider | ID | Instance Format | API Type | Port | Status |
|----------|-----|----------------|----------|------|--------|
| Ollama | `ollama` | `ollama:default` | Ollama Native | 11434 | âœ… |
| llama.cpp | `llamacpp` | `llamacpp:instance-name` | OpenAI Compatible | 8080/11435 | âœ… |
| LM Studio | `lmstudio` | `lmstudio:default` | OpenAI Compatible | 1234 | âœ… |
| OpenAI | `openai` | N/A | OpenAI API | N/A | âœ… |
| Anthropic | `anthropic` | N/A | Anthropic API | N/A | â³ |

---

## ğŸ’¡ å…³é”®è®¾è®¡å†³ç­–

### 1. Instance-Based Provider Format

ä½¿ç”¨ `provider:instance` æ ¼å¼æ”¯æŒå¤šå®ä¾‹ï¼š
- `llamacpp:qwen3-coder-30b` â†’ ç«¯å£ 11435
- `llamacpp:glm47flash-q8` â†’ ç«¯å£ 11434

å¥½å¤„ï¼š
- æ”¯æŒåŒä¸€ provider çš„å¤šä¸ªè¿è¡Œå®ä¾‹
- å¯ä»¥ä¸ºä¸åŒæ¨¡å‹é…ç½®ä¸åŒçš„ç«¯å£å’Œå‚æ•°
- å‘åå…¼å®¹ç®€å•çš„ provider åç§°ï¼ˆ`llamacpp`ï¼‰

### 2. OpenAI-Compatible API ç»Ÿä¸€å¤„ç†

llama.cpp å’Œ LM Studio éƒ½å®ç°äº† OpenAI å…¼å®¹çš„ APIï¼Œå› æ­¤ï¼š
- å¤ç”¨ `OpenAIChatAdapter` è€Œä¸æ˜¯åˆ›å»ºæ–°çš„ adapter
- åªéœ€è¦ä¿®æ”¹ `base_url` å’Œè·³è¿‡ API key æ£€æŸ¥
- å‡å°‘ä»£ç é‡å¤ï¼Œæé«˜å¯ç»´æŠ¤æ€§

### 3. Fallback æœºåˆ¶

```python
provider = session.metadata.get("provider")
if not provider:
    provider = "ollama" if model_route == "local" else "openai"
```

- ä¼˜å…ˆä½¿ç”¨ session metadata ä¸­çš„ provider
- å¦‚æœæœªæŒ‡å®šï¼Œå›é€€åˆ°é»˜è®¤å€¼
- ä¿è¯å‘åå…¼å®¹æ€§

---

## ğŸš¨ æ³¨æ„äº‹é¡¹

### 1. API Key å¤„ç†

å¯¹äºæœ¬åœ°æœåŠ¡ï¼ˆllama.cpp, lmstudioï¼‰ï¼Œ`OpenAIChatAdapter` ä½¿ç”¨ `api_key="dummy"`ï¼š
- OpenAI åº“éœ€è¦æä¾› API key å‚æ•°
- æœ¬åœ°æœåŠ¡é€šå¸¸ä¸éªŒè¯ API key
- ä½¿ç”¨å ä½ç¬¦ "dummy" é¿å…æŠ¥é”™

### 2. Health Check ä¼˜å…ˆçº§

```python
# 1. ä¼˜å…ˆå°è¯• /health ç«¯ç‚¹ï¼ˆllama.cpp æ ‡å‡†ï¼‰
health_url = base_url.replace("/v1", "/health")

# 2. å›é€€åˆ° /v1/models ç«¯ç‚¹ï¼ˆOpenAI å…¼å®¹ï¼‰
models_url = f"{base_url}/models"
```

### 3. Registry è®¿é—®

```python
registry = ProviderRegistry.get_instance()  # å•ä¾‹æ¨¡å¼
provider = registry.get(f"llamacpp:{instance_id}")  # è·å– provider å¯¹è±¡
```

ä¸è¦ä½¿ç”¨ `registry.get_instance(provider_id)` - è¿™æ˜¯é”™è¯¯çš„ï¼

---

## ğŸ“‹ ç›¸å…³æ–‡æ¡£

- **Chat æ¶ˆæ¯å¤„ç†ä¿®å¤**: `docs/bugfixes/CHAT_MESSAGE_HANDLING_FIX.md`
- **æ•°æ®åº“è¿ç§»ä¿®å¤**: `docs/bugfixes/DATABASE_MIGRATION_V08_FIX.md`
- **Provider æ¶æ„**: `agentos/providers/README.md`
- **OpenAI å…¼å®¹ API**: https://platform.openai.com/docs/api-reference/chat

---

---

## ğŸ”§ äºŒæ¬¡ä¿®å¤ï¼šAuto-Selection æœºåˆ¶

### é—®é¢˜æè¿°

ç”¨æˆ·æŠ¥å‘Šç»§ç»­æ”¶åˆ° `âš ï¸ Model unavailable: âœ— Service error (404)` é”™è¯¯ã€‚

**æ ¹æœ¬åŸå› **ï¼š
- WebUI å‰ç«¯å‘é€ `provider="llamacpp"`ï¼ˆä¸å¸¦ instanceï¼‰
- åç«¯ `get_adapter("llamacpp")` ä½¿ç”¨é»˜è®¤ç«¯å£ 8080
- å®é™…ä¸Šæ²¡æœ‰æœåŠ¡è¿è¡Œåœ¨ 8080ï¼ŒæœåŠ¡è¿è¡Œåœ¨ 11435ï¼ˆ`llamacpp:qwen3-coder-30b`ï¼‰

### è§£å†³æ–¹æ¡ˆï¼šProvider Auto-Selection

ä¿®æ”¹ `get_adapter()` åœ¨æ²¡æœ‰æŒ‡å®š instance æ—¶ï¼Œè‡ªåŠ¨æŸ¥æ‰¾å¹¶é€‰æ‹©å¯ç”¨çš„ instanceï¼š

```python
# Handle llama.cpp (OpenAI-compatible)
elif provider_type == "llamacpp":
    model = model or "local-model"
    base_url = None

    # Get actual endpoint from registry
    try:
        from agentos.providers.registry import ProviderRegistry
        registry = ProviderRegistry.get_instance()

        if instance_id:
            # Specific instance requested
            provider_obj = registry.get(f"llamacpp:{instance_id}")
            if provider_obj and hasattr(provider_obj, 'endpoint'):
                base_url = provider_obj.endpoint
        else:
            # No instance specified - find any available llamacpp instance
            from agentos.providers.base import ProviderState
            import asyncio
            all_providers = registry.list_all()
            for p in all_providers:
                if p.id.startswith("llamacpp:"):
                    # Check if this provider is ready
                    status = p.get_cached_status()
                    if not status:
                        # No cached status, probe it
                        try:
                            status = asyncio.run(p.probe())
                        except:
                            continue

                    if status and status.state == ProviderState.READY:
                        base_url = p.endpoint
                        logger.info(f"Auto-selected llamacpp instance: {p.id} at {base_url}")
                        break

        if not base_url:
            # Fallback to default port
            base_url = "http://127.0.0.1:8080"
            logger.warning(f"No llamacpp instance found, using default: {base_url}")

    except Exception as e:
        logger.warning(f"Failed to get llamacpp endpoint: {e}", exc_info=True)
        base_url = "http://127.0.0.1:8080"

    # llama.cpp uses OpenAI-compatible API
    return OpenAIChatAdapter(model=model, base_url=f"{base_url}/v1", api_key="dummy")
```

### Auto-Selection æµ‹è¯•ç»“æœ

```bash
$ python3 test_auto_select.py

Testing auto-selection of llamacpp instance...

1. get_adapter('llamacpp') - should auto-select available instance
   âœ“ Adapter created
   - Base URL: http://127.0.0.1:11435/v1
   - Model: local-model
   - Health: âœ“ Local Model (local-model)
   âœ“ Correctly auto-selected instance on port 11435

2. get_adapter('llamacpp', 'Qwen3-Coder-30B-A3B-Instruct-UD-Q8_K_XL.gguf')
   âœ“ Adapter created
   - Base URL: http://127.0.0.1:11435/v1
   - Model: Qwen3-Coder-30B-A3B-Instruct-UD-Q8_K_XL.gguf
   - Response: OK
   âœ“ Generate works

============================================================
âœ“ AUTO-SELECTION TEST PASSED
============================================================
```

### Auto-Selection ä¼˜åŠ¿

1. **ç”¨æˆ·å‹å¥½**ï¼šå‰ç«¯ä¸éœ€è¦çŸ¥é“å…·ä½“çš„ instance ID
2. **è‡ªåŠ¨å‘ç°**ï¼šç³»ç»Ÿè‡ªåŠ¨æ‰¾åˆ°å¯ç”¨çš„æœåŠ¡å®ä¾‹
3. **å¥å£®æ€§**ï¼šå¦‚æœé¦–é€‰å®ä¾‹ä¸å¯ç”¨ï¼Œä¼šå°è¯•å…¶ä»–å®ä¾‹
4. **å‘åå…¼å®¹**ï¼šä»ç„¶æ”¯æŒæ˜¾å¼æŒ‡å®š instanceï¼ˆ`llamacpp:instance-name`ï¼‰

---

---

## ğŸ”§ ä¸‰æ¬¡ä¿®å¤ï¼šModel-Based Instance Routing

### é—®é¢˜æè¿°

ç”¨æˆ·æŠ¥å‘Šï¼š
- é€‰æ‹© **qwen2.5** æ¨¡å‹ â†’ æ­£ç¡®è¿”å› "æˆ‘æ˜¯Qwen" âœ…
- é€‰æ‹© **qwen3** æ¨¡å‹ â†’ é”™è¯¯è¿”å› "æˆ‘æ˜¯Claude" âŒ

### æ ¹æœ¬åŸå› 

**Instance å’Œ Model çš„æ˜ å°„é—®é¢˜**ï¼š

ç³»ç»Ÿæœ‰ä¸¤ä¸ª llamacpp å®ä¾‹ï¼š
- `llamacpp:qwen3-coder-30b` @ 11435 â†’ è¿è¡Œ `Qwen3-Coder-30B-A3B-Instruct-UD-Q8_K_XL.gguf`
- `llamacpp:qwen2.5-coder-7b` @ 11436 â†’ è¿è¡Œ `qwen2.5-coder-7b-instruct-q8_0.gguf`

**ä¹‹å‰çš„ auto-selection é€»è¾‘**ï¼š
```python
# é€‰æ‹©ç¬¬ä¸€ä¸ª READY çš„ instanceï¼ˆæ€»æ˜¯ 11435ï¼‰
for p in all_providers:
    if p.id.startswith("llamacpp:"):
        if status.state == ProviderState.READY:
            base_url = p.endpoint  # 11435
            break
```

**é—®é¢˜**ï¼š
1. ç”¨æˆ·é€‰æ‹© qwen2.5 æ¨¡å‹
2. ç³»ç»Ÿé€‰æ‹©ç¬¬ä¸€ä¸ªå¯ç”¨å®ä¾‹ï¼ˆ11435 qwen3ï¼‰
3. å‘é€ model="qwen2.5-coder-7b-instruct-q8_0.gguf" åˆ° 11435
4. llama.cpp æ‰¾ä¸åˆ°è¿™ä¸ªæ¨¡å‹ï¼Œå¯èƒ½ fallback åˆ° Claude API

### è§£å†³æ–¹æ¡ˆï¼šModel-Based Instance Selection

ä¿®æ”¹ `get_adapter()` é€»è¾‘ï¼Œæ ¹æ® model å‚æ•°æŸ¥æ‰¾æ­£ç¡®çš„ instanceï¼š

```python
# If model is specified, find instance that has this model
if model:
    logger.info(f"Looking for llamacpp instance with model: {model}")
    for p in llamacpp_providers:
        # Check if provider is ready
        status = p.get_cached_status()
        if status and status.state == ProviderState.READY:
            # Check if this instance has the model
            try:
                response = requests.get(f"{p.endpoint}/v1/models", timeout=2)
                if response.status_code == 200:
                    data = response.json()
                    models = [m.get("id") for m in data.get("data", [])]

                    if model in models:
                        base_url = p.endpoint
                        logger.info(f"âœ“ Found model '{model}' in instance: {p.id} at {base_url}")
                        break
            except Exception as e:
                logger.debug(f"Failed to check models for {p.id}: {e}")
                continue

# Fallback: select first available instance (if model not found)
if not base_url:
    logger.warning(f"Model '{model}' not found in any instance, using first available")
    # ... fallback logic ...
```

### æµ‹è¯•ç»“æœ

```bash
Model-Based Instance Routing Test

TEST 1: qwen2.5-coder-7b-instruct-q8_0.gguf
  Base URL: http://127.0.0.1:11436/v1  â† æ­£ç¡®ï¼
  âœ“ Correctly selected port 11436 for qwen2.5
  Response: æˆ‘æ˜¯Qwenï¼Œä¸€ä¸ªç”±é˜¿é‡Œäº‘å¼€å‘çš„è¯­è¨€æ¨¡å‹ã€‚
  âœ“ Response is from Qwen model

TEST 2: Qwen3-Coder-30B-A3B-Instruct-UD-Q8_K_XL.gguf
  Base URL: http://127.0.0.1:11435/v1  â† æ­£ç¡®ï¼
  âœ“ Correctly selected port 11435 for qwen3
  Response: æˆ‘æ˜¯é€šä¹‰åƒé—®ï¼Œç”±é€šä¹‰å®éªŒå®¤ç ”å‘çš„è¶…å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ã€‚
  âœ“ Response is from Qwen model (not Claude)

âœ“ ALL ROUTING TESTS PASSED

Summary:
- qwen2.5 model â†’ port 11436 âœ“
- qwen3 model â†’ port 11435 âœ“
- Both models respond correctly âœ“
```

### ä¿®å¤ä¼˜åŠ¿

1. **æ™ºèƒ½è·¯ç”±**ï¼šæ ¹æ® model å‚æ•°è‡ªåŠ¨é€‰æ‹©æ­£ç¡®çš„ instance
2. **åŠ¨æ€å‘ç°**ï¼šæŸ¥è¯¢æ¯ä¸ª instance çš„ /v1/models ç«¯ç‚¹
3. **å¥å£® Fallback**ï¼šå¦‚æœæ‰¾ä¸åˆ° modelï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨ instance
4. **æ”¯æŒå¤šå®ä¾‹**ï¼šå¯ä»¥åŒæ—¶è¿è¡Œå¤šä¸ªä¸åŒçš„æ¨¡å‹

---

**ä¿®å¤å®Œæˆæ—¶é—´**: 2026-01-28
**æµ‹è¯•çŠ¶æ€**: âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ˆåŒ…æ‹¬ Model-Based Routingï¼‰
**å—å½±å“çš„æ–‡ä»¶**:
- `agentos/core/chat/engine.py` - æ·»åŠ  metadata æ—¥å¿—
- `agentos/core/chat/adapters.py` - Model-based instance selection
- `agentos/webui/websocket/chat.py` - è°ƒç”¨ update_session_metadata

**Final E2E Test**: âœ… å…¨éƒ¨é€šè¿‡
- qwen2.5 æ¨¡å‹è·¯ç”±æ­£ç¡® âœ…
- qwen3 æ¨¡å‹è·¯ç”±æ­£ç¡® âœ…
- Session metadata æ›´æ–°æ­£å¸¸ âœ…
