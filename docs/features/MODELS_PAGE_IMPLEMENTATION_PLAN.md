# Models ç®¡ç†é¡µé¢å®ç°æ–¹æ¡ˆ

## ğŸ“‹ æ¦‚è¿°

åœ¨ WebUI çš„ Settings éƒ¨åˆ†æ·»åŠ  Models é¡µé¢ï¼Œç”¨äºä¸‹è½½å’Œç®¡ç†æœ¬åœ° AI æ¨¡å‹ï¼ˆOllamaã€llama.cppï¼‰ã€‚

## ğŸ¯ åŠŸèƒ½éœ€æ±‚

### æ ¸å¿ƒåŠŸèƒ½
1. **æ¨¡å‹åˆ—è¡¨å±•ç¤º** - æ˜¾ç¤ºå·²å®‰è£…çš„æ¨¡å‹ï¼ˆå¡ç‰‡å¼å¸ƒå±€ï¼‰
2. **æ¨¡å‹ä¸‹è½½** - ä»æ¨èåˆ—è¡¨æˆ–è‡ªå®šä¹‰åç§°ä¸‹è½½æ¨¡å‹
3. **ä¸‹è½½è¿›åº¦** - å®æ—¶æ˜¾ç¤ºä¸‹è½½è¿›åº¦æ¡
4. **æ¨¡å‹ç®¡ç†** - åˆ é™¤ã€æŸ¥çœ‹è¯¦æƒ…
5. **æœåŠ¡çŠ¶æ€** - æ˜¾ç¤º Ollama/llama.cpp æœåŠ¡è¿è¡ŒçŠ¶æ€

### é¡µé¢äº¤äº’æµç¨‹
```
ç”¨æˆ·æ‰“å¼€ Models é¡µé¢
    â†“
æ˜¾ç¤ºæœåŠ¡çŠ¶æ€ + å·²å®‰è£…æ¨¡å‹åˆ—è¡¨
    â†“
ç‚¹å‡» [+ Download] æŒ‰é’®
    â†“
å¼¹å‡ºä¸‹è½½å¯¹è¯æ¡†ï¼ˆæ¨èæ¨¡å‹ + è‡ªå®šä¹‰è¾“å…¥ï¼‰
    â†“
é€‰æ‹©æ¨¡å‹å¹¶ç¡®è®¤ä¸‹è½½
    â†“
æ˜¾ç¤ºä¸‹è½½è¿›åº¦æ¡ï¼ˆå®æ—¶æ›´æ–°ï¼‰
    â†“
ä¸‹è½½å®Œæˆåè‡ªåŠ¨åˆ·æ–°æ¨¡å‹åˆ—è¡¨
```

## ğŸ“‚ æ–‡ä»¶ç»“æ„

```
agentos/
â”œâ”€â”€ webui/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ models.py                    # æ–°å»º - Models API è·¯ç”±
â”‚   â”œâ”€â”€ static/
â”‚   â”‚   â”œâ”€â”€ css/
â”‚   â”‚   â”‚   â””â”€â”€ models.css              # æ–°å»º - Models é¡µé¢æ ·å¼
â”‚   â”‚   â””â”€â”€ js/
â”‚   â”‚       â””â”€â”€ views/
â”‚   â”‚           â””â”€â”€ ModelsView.js       # æ–°å»º - Models è§†å›¾ç±»
â”‚   â””â”€â”€ templates/
â”‚       â””â”€â”€ index.html                   # ä¿®æ”¹ - æ·»åŠ  Models èœå•é¡¹
â””â”€â”€ cli/
    â””â”€â”€ provider_checker.py              # å·²æœ‰ - æ‰©å±•è¿›åº¦å›è°ƒåŠŸèƒ½
```

## ğŸ¨ UI è®¾è®¡

### 1. å¯¼èˆªèœå•é¡¹ï¼ˆæ·»åŠ åˆ° Settings éƒ¨åˆ†ï¼‰
```html
<!-- åœ¨ Extensions ä¸‹æ–¹æ·»åŠ  -->
<a href="#" class="nav-item" data-view="models">
    <svg class="w-5 h-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
        <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2"
              d="M19 11H5m14 0a2 2 0 012 2v6a2 2 0 01-2 2H5a2 2 0 01-2-2v-6a2 2 0 012-2m14 0V9a2 2 0 00-2-2M5 11V9a2 2 0 012-2m0 0V5a2 2 0 012-2h6a2 2 0 012 2v2M7 7h10" />
    </svg>
    <span>Models</span>
</a>
```

### 2. é¡µé¢ä¸»ä½“ç»“æ„
```html
<div class="models-view">
    <!-- Header -->
    <div class="view-header">
        <div>
            <h1>Models</h1>
            <p class="text-sm text-gray-600 mt-1">Download and manage local AI models</p>
        </div>
        <div class="header-actions">
            <button class="btn-primary" id="btnDownloadModel">
                <span class="icon"><span class="material-icons md-18">download</span></span>
                Download Model
            </button>
        </div>
    </div>

    <!-- Service Status Section -->
    <div class="status-section">
        <h2>Service Status</h2>
        <div class="status-grid">
            <!-- Ollama Status -->
            <div class="status-card">
                <div class="status-header">
                    <span class="status-indicator status-running"></span>
                    <h3>Ollama</h3>
                </div>
                <p class="status-info">v0.15.2 (Running)</p>
                <div class="status-actions">
                    <button class="btn-sm btn-secondary">Start</button>
                    <button class="btn-sm btn-secondary">Stop</button>
                </div>
            </div>

            <!-- llama.cpp Status -->
            <div class="status-card">
                <div class="status-header">
                    <span class="status-indicator status-stopped"></span>
                    <h3>llama.cpp</h3>
                </div>
                <p class="status-info">Not Available</p>
            </div>
        </div>
    </div>

    <!-- Download Progress (shown when downloading) -->
    <div id="downloadProgressContainer" style="display: none;"></div>

    <!-- Models Grid -->
    <div class="table-section">
        <h2>Installed Models</h2>
        <div id="modelsGrid" class="models-grid">
            <!-- Model cards will be rendered here -->
        </div>
    </div>
</div>
```

### 3. æ¨¡å‹å¡ç‰‡è®¾è®¡
```html
<div class="model-card">
    <div class="model-card-header">
        <div class="model-icon">ğŸ¤–</div>
        <div class="model-info">
            <h3>qwen2.5:7b</h3>
            <div class="model-meta">
                <span class="model-params">7B params</span>
                <span class="model-size">4.7 GB</span>
            </div>
        </div>
    </div>
    <div class="model-card-body">
        <p class="model-description">
            Qwen 2.5 - ä¸­æ–‡ä¼˜åŒ–çš„å¤§è¯­è¨€æ¨¡å‹ï¼Œé€‚åˆä¸­æ–‡å¯¹è¯å’Œä»£ç ç”Ÿæˆ
        </p>
        <div class="model-tags">
            <span class="tag">chat</span>
            <span class="tag">code</span>
            <span class="tag">chinese</span>
        </div>
    </div>
    <div class="model-card-actions">
        <button class="btn-primary" data-action="run">Run</button>
        <button class="btn-secondary" data-action="info">Info</button>
        <button class="btn-delete" data-action="delete">Delete</button>
    </div>
</div>
```

## ğŸ”Œ API æ¥å£è®¾è®¡

### 1. è·å–å·²å®‰è£…æ¨¡å‹åˆ—è¡¨
```
GET /api/models/list

Response:
{
    "models": [
        {
            "name": "qwen2.5:7b",
            "provider": "ollama",
            "size": "4.7 GB",
            "size_bytes": 5046586573,
            "params": "7B",
            "family": "qwen2.5",
            "format": "gguf",
            "modified_at": "2024-01-15T10:30:00Z",
            "digest": "sha256:abc123...",
            "details": {
                "parent_model": "",
                "format": "gguf",
                "family": "qwen2.5",
                "families": ["qwen"],
                "parameter_size": "7.6B",
                "quantization_level": "Q4_0"
            }
        }
    ],
    "total": 3
}
```

### 2. è·å–å¯ä¸‹è½½æ¨¡å‹åˆ—è¡¨ï¼ˆæ¨èæ¨¡å‹ï¼‰
```
GET /api/models/available

Response:
{
    "recommended": [
        {
            "name": "qwen2.5:7b",
            "display_name": "Qwen 2.5 (7B)",
            "description": "ä¸­æ–‡ä¼˜åŒ–çš„å¤§è¯­è¨€æ¨¡å‹ï¼Œé€‚åˆä¸­æ–‡å¯¹è¯å’Œä»£ç ç”Ÿæˆ",
            "size": "4.7 GB",
            "params": "7B",
            "tags": ["chat", "code", "chinese"],
            "category": "general"
        },
        {
            "name": "llama3.2:3b",
            "display_name": "Llama 3.2 (3B)",
            "description": "å¿«é€Ÿå“åº”ï¼Œé€‚åˆæ—¥å¸¸å¯¹è¯",
            "size": "2.0 GB",
            "params": "3B",
            "tags": ["chat", "fast"],
            "category": "general"
        },
        {
            "name": "llama3.2:1b",
            "display_name": "Llama 3.2 (1B)",
            "description": "è¶…è½»é‡çº§ï¼Œå¿«é€Ÿå“åº”",
            "size": "1.3 GB",
            "params": "1B",
            "tags": ["chat", "fast", "lightweight"],
            "category": "general"
        },
        {
            "name": "gemma2:2b",
            "display_name": "Gemma 2 (2B)",
            "description": "Google å¼€æºçš„å°å‹æ¨¡å‹",
            "size": "1.6 GB",
            "params": "2B",
            "tags": ["chat"],
            "category": "general"
        },
        {
            "name": "qwen2.5-coder:7b",
            "display_name": "Qwen 2.5 Coder (7B)",
            "description": "ä»£ç ç”Ÿæˆä¸“ç”¨æ¨¡å‹",
            "size": "4.7 GB",
            "params": "7B",
            "tags": ["code"],
            "category": "coding"
        }
    ]
}
```

### 3. ä¸‹è½½æ¨¡å‹
```
POST /api/models/pull

Request:
{
    "model_name": "qwen2.5:7b",
    "provider": "ollama"  // å¯é€‰ï¼Œé»˜è®¤ ollama
}

Response:
{
    "pull_id": "pull_abc123",
    "model_name": "qwen2.5:7b",
    "status": "PULLING"
}
```

### 4. æŸ¥è¯¢ä¸‹è½½è¿›åº¦
```
GET /api/models/pull/{pull_id}

Response:
{
    "pull_id": "pull_abc123",
    "model_name": "qwen2.5:7b",
    "status": "PULLING",  // PULLING, COMPLETED, FAILED
    "progress": 75,
    "current_layer": 3,
    "total_layers": 4,
    "downloaded_bytes": 3543891968,
    "total_bytes": 4726735872,
    "current_status": "pulling manifest",
    "error": null
}
```

### 5. åˆ é™¤æ¨¡å‹
```
DELETE /api/models/{provider}/{model_name}

Response:
{
    "success": true,
    "message": "Model qwen2.5:7b deleted successfully"
}
```

### 6. è·å–æœåŠ¡çŠ¶æ€
```
GET /api/models/status

Response:
{
    "ollama": {
        "available": true,
        "running": true,
        "version": "0.15.2",
        "host": "http://localhost:11434"
    },
    "llama_cpp": {
        "available": false,
        "running": false,
        "info": "Command not found"
    }
}
```

## ğŸ’¾ æ•°æ®åº“æ‰©å±•

åœ¨ models è¡¨ä¸­æ·»åŠ ä¸‹è½½è®°å½•ï¼ˆå¯é€‰ï¼Œç”¨äºå†å²è¿½è¸ªï¼‰ï¼š

```sql
-- æ¨¡å‹ä¸‹è½½è®°å½•è¡¨
CREATE TABLE IF NOT EXISTS model_pulls (
    pull_id TEXT PRIMARY KEY,
    model_name TEXT NOT NULL,
    provider TEXT NOT NULL,  -- ollama, llama_cpp
    status TEXT NOT NULL,    -- PULLING, COMPLETED, FAILED
    progress INTEGER DEFAULT 0,
    started_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    completed_at TIMESTAMP,
    error TEXT,
    metadata JSON
);
```

## ğŸ¯ å®ç°ä¼˜å…ˆçº§

### Phase 1: åŸºç¡€åŠŸèƒ½ (MVP)
- [ ] åˆ›å»º ModelsView.js å’Œ models.css
- [ ] æ·»åŠ å¯¼èˆªèœå•é¡¹
- [ ] å®ç° API è·¯ç”±ï¼ˆmodels.pyï¼‰
- [ ] å®ç°æ¨¡å‹åˆ—è¡¨å±•ç¤º
- [ ] å®ç°æ¨¡å‹ä¸‹è½½åŠŸèƒ½
- [ ] å®ç°ä¸‹è½½è¿›åº¦æ˜¾ç¤º

### Phase 2: å¢å¼ºåŠŸèƒ½
- [ ] æœåŠ¡çŠ¶æ€ç›‘æ§å’Œæ§åˆ¶
- [ ] æ¨¡å‹åˆ é™¤åŠŸèƒ½
- [ ] æ¨¡å‹è¯¦æƒ…æŸ¥çœ‹
- [ ] è‡ªå®šä¹‰æ¨¡å‹ä¸‹è½½
- [ ] æ‰¹é‡æ“ä½œæ”¯æŒ

### Phase 3: é«˜çº§åŠŸèƒ½
- [ ] llama.cpp æ¨¡å‹æ”¯æŒ
- [ ] æ¨¡å‹æ€§èƒ½æµ‹è¯•
- [ ] æ¨¡å‹æ¨èç®—æ³•
- [ ] æ¨¡å‹ç‰ˆæœ¬ç®¡ç†

## ğŸ”§ æŠ€æœ¯å®ç°ç»†èŠ‚

### 1. ä¸‹è½½è¿›åº¦è¿½è¸ªæœºåˆ¶

ä½¿ç”¨åå°çº¿ç¨‹ + è½®è¯¢æœºåˆ¶ï¼š

```python
# åç«¯
import threading
import uuid

# å…¨å±€å­˜å‚¨ä¸‹è½½è¿›åº¦
_pull_progress = {}

def pull_model_background(pull_id: str, model_name: str):
    """åå°ä¸‹è½½æ¨¡å‹"""
    try:
        _pull_progress[pull_id] = {
            "status": "PULLING",
            "progress": 0,
            "model_name": model_name
        }

        # è°ƒç”¨ ollama pullï¼Œè§£æè¾“å‡ºè¿›åº¦
        process = subprocess.Popen(
            ["ollama", "pull", model_name],
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            text=True
        )

        for line in process.stdout:
            # è§£æè¿›åº¦ï¼ˆOllama è¾“å‡ºæ ¼å¼ï¼‰
            # pulling manifest
            # pulling sha256:... 100% â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
            progress = parse_progress(line)
            _pull_progress[pull_id]["progress"] = progress
            _pull_progress[pull_id]["current_status"] = line.strip()

        if process.returncode == 0:
            _pull_progress[pull_id]["status"] = "COMPLETED"
            _pull_progress[pull_id]["progress"] = 100
        else:
            _pull_progress[pull_id]["status"] = "FAILED"

    except Exception as e:
        _pull_progress[pull_id]["status"] = "FAILED"
        _pull_progress[pull_id]["error"] = str(e)
```

```javascript
// å‰ç«¯
class ModelsView {
    async pullModel(modelName) {
        // 1. å‘èµ·ä¸‹è½½è¯·æ±‚
        const response = await fetch('/api/models/pull', {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({ model_name: modelName })
        });

        const { pull_id } = await response.json();

        // 2. æ˜¾ç¤ºè¿›åº¦æ¡
        this.showPullProgress(pull_id, modelName);

        // 3. å¼€å§‹è½®è¯¢è¿›åº¦
        this.pollPullProgress(pull_id);
    }

    async pollPullProgress(pullId) {
        const interval = setInterval(async () => {
            const response = await fetch(`/api/models/pull/${pullId}`);
            const data = await response.json();

            // æ›´æ–°è¿›åº¦æ¡
            this.updateProgressBar(pullId, data.progress);

            // æ£€æŸ¥æ˜¯å¦å®Œæˆ
            if (data.status === 'COMPLETED') {
                clearInterval(interval);
                this.showNotification('Model downloaded successfully', 'success');
                this.loadModels(); // åˆ·æ–°åˆ—è¡¨
            } else if (data.status === 'FAILED') {
                clearInterval(interval);
                this.showNotification(`Download failed: ${data.error}`, 'error');
            }
        }, 500); // æ¯500msè½®è¯¢ä¸€æ¬¡
    }
}
```

### 2. æ¨¡å‹å¤§å°æ ¼å¼åŒ–

```javascript
function formatBytes(bytes) {
    if (bytes === 0) return '0 B';
    const k = 1024;
    const sizes = ['B', 'KB', 'MB', 'GB', 'TB'];
    const i = Math.floor(Math.log(bytes) / Math.log(k));
    return Math.round(bytes / Math.pow(k, i) * 100) / 100 + ' ' + sizes[i];
}
```

### 3. æœåŠ¡çŠ¶æ€å®æ—¶ç›‘æ§

```javascript
class ModelsView {
    constructor() {
        this.statusCheckInterval = null;
    }

    async render(container) {
        // ...æ¸²æŸ“é¡µé¢

        // å¯åŠ¨çŠ¶æ€æ£€æŸ¥
        this.startStatusCheck();
    }

    startStatusCheck() {
        // æ¯5ç§’æ£€æŸ¥ä¸€æ¬¡æœåŠ¡çŠ¶æ€
        this.statusCheckInterval = setInterval(async () => {
            await this.updateServiceStatus();
        }, 5000);
    }

    destroy() {
        if (this.statusCheckInterval) {
            clearInterval(this.statusCheckInterval);
        }
    }

    async updateServiceStatus() {
        const response = await fetch('/api/models/status');
        const data = await response.json();

        // æ›´æ–° UI çŠ¶æ€æŒ‡ç¤ºå™¨
        this.updateStatusIndicators(data);
    }
}
```

## ğŸ¨ æ ·å¼è§„èŒƒ

éµå¾ª Extensions é¡µé¢çš„æ ·å¼è®¾è®¡ï¼š

```css
/* models.css */

/* ç»§æ‰¿ extensions.css çš„åŸºç¡€æ ·å¼ */
.models-view {
    padding: 20px;
}

/* æœåŠ¡çŠ¶æ€éƒ¨åˆ† */
.status-section {
    margin-bottom: 24px;
    padding: 20px;
    background: white;
    border: 1px solid #e5e7eb;
    border-radius: 0.75rem;
}

.status-grid {
    display: grid;
    grid-template-columns: repeat(auto-fill, minmax(300px, 1fr));
    gap: 1rem;
    margin-top: 1rem;
}

.status-card {
    padding: 1.5rem;
    background: #f9fafb;
    border: 1px solid #e5e7eb;
    border-radius: 0.5rem;
}

.status-indicator {
    display: inline-block;
    width: 12px;
    height: 12px;
    border-radius: 50%;
    margin-right: 0.5rem;
}

.status-indicator.status-running {
    background: #10b981;
    animation: pulse 2s infinite;
}

.status-indicator.status-stopped {
    background: #6b7280;
}

/* æ¨¡å‹å¡ç‰‡ç½‘æ ¼ */
.models-grid {
    display: grid;
    grid-template-columns: repeat(auto-fill, minmax(320px, 1fr));
    gap: 1.5rem;
}

/* æ¨¡å‹å¡ç‰‡æ ·å¼ï¼ˆä¸ extension-card å¯¹é½ï¼‰*/
.model-card {
    background: white;
    border: 1px solid #e5e7eb;
    border-radius: 0.75rem;
    padding: 1.5rem;
    transition: all 0.2s ease;
    display: flex;
    flex-direction: column;
    height: 100%;
}

.model-card:hover {
    border-color: #3b82f6;
    box-shadow: 0 4px 12px rgba(59, 130, 246, 0.1);
}

/* ä¸‹è½½è¿›åº¦æ ·å¼ï¼ˆä¸ install-progress å¯¹é½ï¼‰*/
.download-progress {
    padding: 1.5rem;
    background: white;
    border: 1px solid #e5e7eb;
    border-radius: 0.75rem;
    margin-bottom: 2rem;
}

.progress-bar {
    width: 100%;
    height: 8px;
    background: #e5e7eb;
    border-radius: 9999px;
    overflow: hidden;
    margin: 0.75rem 0;
}

.progress-fill {
    height: 100%;
    background: linear-gradient(90deg, #3b82f6 0%, #8b5cf6 100%);
    transition: width 0.3s ease;
    border-radius: 9999px;
}

/* å“åº”å¼è®¾è®¡ */
@media (max-width: 768px) {
    .models-grid {
        grid-template-columns: 1fr;
    }

    .status-grid {
        grid-template-columns: 1fr;
    }
}
```

## ğŸš€ éƒ¨ç½²å’Œæµ‹è¯•

### å¼€å‘ç¯å¢ƒæµ‹è¯•æ­¥éª¤

1. **å¯åŠ¨ Ollama æœåŠ¡**
   ```bash
   ollama serve
   ```

2. **å¯åŠ¨ AgentOS WebUI**
   ```bash
   agentos webui
   ```

3. **è®¿é—® Models é¡µé¢**
   - æ‰“å¼€æµè§ˆå™¨: http://localhost:8000
   - ç‚¹å‡» Settings â†’ Models

4. **æµ‹è¯•ä¸‹è½½åŠŸèƒ½**
   - ç‚¹å‡» [+ Download Model]
   - é€‰æ‹© llama3.2:1b (æœ€å°æ¨¡å‹ï¼Œå¿«é€Ÿæµ‹è¯•)
   - è§‚å¯Ÿä¸‹è½½è¿›åº¦
   - éªŒè¯ä¸‹è½½å®Œæˆåæ¨¡å‹å‡ºç°åœ¨åˆ—è¡¨ä¸­

### å•å…ƒæµ‹è¯•

```python
# tests/integration/api/test_models_api.py

import pytest
from agentos.webui.api.models import router

def test_list_models():
    """æµ‹è¯•è·å–æ¨¡å‹åˆ—è¡¨"""
    response = client.get("/api/models/list")
    assert response.status_code == 200
    data = response.json()
    assert "models" in data

def test_pull_model():
    """æµ‹è¯•ä¸‹è½½æ¨¡å‹"""
    response = client.post("/api/models/pull", json={
        "model_name": "llama3.2:1b"
    })
    assert response.status_code == 200
    data = response.json()
    assert "pull_id" in data

def test_get_pull_progress():
    """æµ‹è¯•æŸ¥è¯¢ä¸‹è½½è¿›åº¦"""
    # å…ˆå‘èµ·ä¸‹è½½
    pull_response = client.post("/api/models/pull", json={
        "model_name": "llama3.2:1b"
    })
    pull_id = pull_response.json()["pull_id"]

    # æŸ¥è¯¢è¿›åº¦
    progress_response = client.get(f"/api/models/pull/{pull_id}")
    assert progress_response.status_code == 200
    data = progress_response.json()
    assert "progress" in data
```

## ğŸ“ åç»­ä¼˜åŒ–æ–¹å‘

1. **æ¨¡å‹æ¨èç³»ç»Ÿ** - æ ¹æ®ç”¨æˆ·ç¡¬ä»¶é…ç½®æ¨èåˆé€‚çš„æ¨¡å‹
2. **æ¨¡å‹æ€§èƒ½æµ‹è¯•** - æä¾›æ¨¡å‹æ€§èƒ½åŸºå‡†æµ‹è¯•åŠŸèƒ½
3. **æ¨¡å‹è½¬æ¢å·¥å…·** - æ”¯æŒ GGUFã€GGML æ ¼å¼è½¬æ¢
4. **å¤š Provider æ”¯æŒ** - æ‰©å±•æ”¯æŒ LM Studioã€Jan.ai ç­‰
5. **æ¨¡å‹å¸‚åœº** - é›†æˆç¤¾åŒºæ¨¡å‹å¸‚åœº
6. **ç¦»çº¿æ¨¡å‹å¯¼å…¥** - æ”¯æŒä»æœ¬åœ°æ–‡ä»¶å¯¼å…¥æ¨¡å‹

## ğŸ¯ æˆåŠŸæŒ‡æ ‡

- âœ… ç”¨æˆ·èƒ½åœ¨ 3 æ¬¡ç‚¹å‡»å†…å®Œæˆæ¨¡å‹ä¸‹è½½
- âœ… ä¸‹è½½è¿›åº¦æ›´æ–°å»¶è¿Ÿ < 1 ç§’
- âœ… é¡µé¢åŠ è½½æ—¶é—´ < 2 ç§’
- âœ… æ”¯æŒåŒæ—¶ä¸‹è½½ 2 ä¸ªä»¥ä¸Šæ¨¡å‹
- âœ… ç§»åŠ¨ç«¯å“åº”å¼å¸ƒå±€å®Œç¾é€‚é…

## ğŸ“š å‚è€ƒèµ„æ–™

- Ollama API æ–‡æ¡£: https://github.com/ollama/ollama/blob/main/docs/api.md
- llama.cpp æ–‡æ¡£: https://github.com/ggerganov/llama.cpp
- AgentOS Extensions è®¾è®¡æ–‡æ¡£: docs/extensions/
